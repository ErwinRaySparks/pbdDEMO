\chapter[Basic Statistics Examples]{Basic Statistics Examples}
\label{sec:statistics_examples}

This section introduces five simple examples and explains a little about computing with 
distributed data directly over MPI. 
These implemented examples/functions are partly
selected from the Cookbook of HPSC website~\citep{hpsc2011} at
\url{http://thirteen-01.stat.iastate.edu/snoweye/hpsc/?item=cookbook}.
Please see more details there.

\section[Monte Carlo Simulation]{Monte Carlo Simulation}
\label{sec:monte_carlo}

\emph{Example:  Compute a numerical approximation for $\pi$.}

The demo command is
\begin{lstlisting}
### At the shell prompt, run the demo with 4 processors by
### (Use Rscript.exe for windows system)
mpiexec -np 4 Rscript -e "demo(monte_carlo,'pbdDEMO',ask=F,echo=F)"
\end{lstlisting}

This is a simple Monte Carlo simulation example for numerically estimating $\pi$.
Suppose we sample $N$ uniform observations $(x_i, y_i)$ inside (or perhaps on the border of) the unit square $[0, 1]\times [0,1]$,
where $i = 1, 2, \ldots, N$.  Then
\begin{equation}
\pi \approx 4\frac{L}{N}
\label{eqn:pi}
\end{equation}
where $0\leq L\leq N$ is the number of observations sampled satisfying
\begin{align}
x_i^2+y_i^2 \leq 1\label{math:circle}
\end{align}
The intuitive explanation for this is strategy which is sometimes given belies a misunderstanding of infinite cardinalities, and infinite processes in general.  We are not \emph{directly} approximating an area through this sampling scheme, because to do so with a finite-point sampling scheme would be madness requiring a transfinite process.  Indeed, let $S_N$ be the collection of elements satisfying inequality~(\ref{math:circle}).  Then note that for each $N\in\mathbb{N}$ that the area of $S_N$ is precisely 0.  Whence,
\begin{align*}
\lim_{N\rightarrow\infty} Area(S_N) = 0
\end{align*}
This bears repeating.  Finite sampling of an uncountable space requires uncountably many such sampling operations to ``fill'' the infinite space.  For a proper treatment of set theoretic constructions, including infinite cardinals, see \citep{kunen}.

One could argue that we are evaluating a ratio of integrals with each using the counting measure, which satisfies technical correctness but is far from clear.  Now yes, indeed, certain facts of area are involved here, but some care should be taken in the discussion as to what exactly justifies our claim in (\ref{eqn:pi}).

In reality, we are evaluating the probability that someone throwing a 0-dimensional ``dart'' at the unit square will have that ``dart'' also land below the arc of the unit circle contained within the unit square.  Formally, let $U_1$ and $U_2$ be random uniform variables, each from the closed unit interval $[0, 1]$.  Define the random variable
\begin{align*}
X &:= 
\begin{cases} 
1\text{, } \hspace{.4cm} U_1^2 + U_2^2 \leq 1\\ 
0\text{, \hspace{.4cm} otherwise}
\end{cases}
\end{align*}
Let $V_i = U_i^2$ for $i=1, 2$. Then the expected value
\begin{align*}
E[X] &= P( V_1 + V_2 \leq 1 )\\
     &= \int_0^1 \int_0^{1-V_1} p(V_1, V_2)dV_2 dV_1\\
     &= \int_0^1 \int_0^{1-V_1} \left( \frac{1}{2\sqrt{V_1}} \right) \left( \frac{1}{2\sqrt{V_2}} \right)dV_2 dV_1\\
     &= \frac{1}{2}\int_0^1 \left(\frac{1-V_1}{V_1}\right)^{1/2}dV_1\\
     &= \frac{1}{2} \left[ 
	V_1\left(\frac{1-V_1}{V_1}\right)^{1/2} 
	 - \frac{1}{2} \arctan\left(\frac{\left(\frac{1-V_1}{V_1}\right)^{1/2} (2V_1-1)}{2(V_1-1)}\right) 
	\right]_{V_1\rightarrow 0}^{V_1\rightarrow 1}\\
     &= \frac{1}{2}\left[ \frac{\pi}{4} +\frac{\pi}{4}  \right]
\end{align*}
and by sampling observations $X_i$ for $i=1,\dots,N$, by the Strong Law of Large Numbers
\begin{align*}
\bar{X}_N \longrightarrow^{\hspace{-.55cm} a.s.} \hspace{.1cm} \frac{\pi}{4} \hspace{.4cm} \text{ as } N\rightarrow \infty
\end{align*}
In other words,
\begin{align*}
P\left(\lim_{N\rightarrow\infty} \bar{X}_N = \frac{\pi}{4}\right) = 1
\end{align*}
Whence,
\begin{align*}
\frac{L}{N}  \longrightarrow^{\hspace{-.55cm} a.s.} \hspace{.1cm} \frac{\pi}{4} \hspace{.4cm} \text{ as } N\rightarrow \infty
\end{align*}

But because no one is going to read that, and if they do they'll just call me a grumpy old man,
\begin{figure}[h]
\centering
 \includegraphics[height=8.5cm, width=8.5cm]{pbdDEMO-include/pics/misleading}
 \caption{Approximating $\pi$ by Monte Carlo methods}\label{pic:dumb}
\end{figure}
the misleading picture you desire can be found in Figure~\ref{pic:dumb}.  And to everyone who found this looking for a homework solution, you're welcome.

The key step of the demo code is in the following block:
\begin{lstlisting}[language=rr,title=R Code]
N.spmd <- 1000
X.spmd <- matrix(runif(N.spmd * 2), ncol = 2)
r.spmd <- sum(rowSums(X.spmd^2) <= 1)
ret <- allreduce(c(N.spmd, r.spmd), op = "sum")
PI <- 4 * ret[2] / ret[1]
comm.print(PI)
\end{lstlisting}

In line 1, we specify sample size in \code{N.spmd} for each processor,
and $N=D\times\mbox{\code{N.spmd}}$ if $D$ processors are executed.
In line 2, we generate samples in \code{X.spmd} for every processor.
In line 3, we compute how many of radios are less than or equal to $1$
for each processors.
In line 4, we call \code{allreduce()} to obtain total numbers across all
processors.
In line 5, we use the Equation~(\ref{eqn:pi}).
Since SPMD, \code{ret} is common on all processors, and so is \code{PI}.




\section[Sample Mean and Sample Variance]{Sample Mean and Sample Variance}
\label{sec:sample_stat}

\emph{Example:  Compute sample mean/variance for distributed data.}

The demo command is
\begin{lstlisting}
### At the shell prompt, run the demo with 4 processors by
### (Use Rscript.exe for windows system)
mpiexec -np 4 Rscript -e "demo(sample_stat,'pbdDEMO',ask=F,echo=F)"
\end{lstlisting}

Suppose $\bx = \{x_1, x_2, \ldots, x_N\}$
are observed samples, and $N$ is potentially very large.
We can distribute $\bx$ in 4 processors, and each processor receives a proportional amount of data. One simple way to compute sample mean $\bar{x}$ and
sample variance $s_x$ is based on the formulas:

\begin{align}
\bar{x} &= \frac{1}{N} \sum_{n = 1}^N x_n \notag \\[.3cm]
        &= \sum_{n = 1}^N \frac{x_n}{N} \label{math:mn}
\end{align}
and
\begin{align}
s_x     &= \frac{1}{N - 1} \sum_{n = 1}^N (x_n - \bar{x})^2 \notag \\[.3cm]
        &= \frac{1}{N - 1} \sum_{n = 1}^N x_n^2 - \frac{2\bar{x}}{N - 1}\sum_{n = 1}^N x_n +  \frac{1}{N - 1}\sum_{n = 1}^N\bar{x}^2 \notag \\[.3cm]
        &= \sum_{n = 1}^N \left(\frac{x^2_n}{N-1}\right) - \frac{N \bar{x}^2}{N-1} \label{math:sd}
\end{align}
where expressions (\ref{math:mn}) and (\ref{math:sd}) are one-pass algorithms,
which are potentially faster than the first expressions,
especially for large $N$. Here, only the first and second moments are implemented, while
the extension of one-pass algorithms to higher order moments is also
possible.

The demo generates random data on 4 processors, then
utilizes the \code{mpi.stat()} function:
\begin{lstlisting}[language=rr,title=R Code]
mpi.stat <- function(x.spmd){
  ### For mean(x).
  N <- allreduce(length(x.spmd), op = "sum")
  bar.x.spmd <- sum(x.spmd / N)
  bar.x <- allreduce(bar.x.spmd, op = "sum")

  ### For var(x).
  s.x.spmd <- sum(x.spmd^2 / (N - 1))
  s.x <- allreduce(s.x.spmd, op = "sum") - bar.x^2 * (N / (N - 1))

  list(mean = bar.x, s = s.x)
} # End of mpi.stat().
\end{lstlisting}
where \code{allreduce()} in \pkg{pbdMPI}~\citep{Chen2012pbdMPIpackage} can
be utilized in this examples to aggregate local information across
all processors.

\section[Binning]{Binning}
\label{sec:binning}

\emph{Example:  Find binning counts for distributed data.}

The demo command is
\begin{lstlisting}
### At the shell prompt, run the demo with 4 processors by
### (Use Rscript.exe for windows system)
mpiexec -np 4 Rscript -e "demo(binning,'pbdDEMO',ask=F,echo=F)"
\end{lstlisting}

Binning is a classical problem in statistics which helps to quickly summarize
the data structure by setting some ``breaks'' between the minimum and maximum values.
This is a particularly useful tool for constructing histograms, as well as categorical data analysis.

The demo generates random data on 4 processors, then
utilizes the \code{mpi.bin()} function:
\begin{lstlisting}[language=rr,title=R Code]
mpi.bin <- function(x.spmd, breaks = pi / 3 * (-3:3)){
  bin.spmd <- table(cut(x.spmd, breaks = breaks))
  bin <- as.array(allreduce(bin.spmd, op = "sum"))
  dimnames(bin) <- dimnames(bin.spmd)
  class(bin) <- class(bin.spmd)
  bin
} # End of mpi.bin().
\end{lstlisting}
This simple implementation utilizes \proglang{R}'s own \code{table()} function to obtain
local counts, then calls \code{allreduce()} to obtain global counts.


\section[Quantile]{Quantile}
\label{sec:quantile}

\emph{Example:  Compute sample quantile order statistics for distributed data.}

The demo command is
\begin{lstlisting}
### At the shell prompt, run the demo with 4 processors by
### (Use Rscript.exe for windows system)
mpiexec -np 4 Rscript -e "demo(quantile,'pbdDEMO',ask=F,echo=F)"
\end{lstlisting}

Another fundamental tool in the statistician's toolbox is finding quantiles.  Quantiles are points taken from the cumulative distribution function which are taken in evenly-spaced periods.  The most common usage is the ``2-quantile'', or the median, though others are of course many others which are useful in summarizing distributions.

This example can be extended to construct Q-Q plots, compute cumulative density function estimates and nonparametric statistics, as well as solve maximum likelihood estimators.

This is perhaps an inefficient implementation to approximate a quantile and is not equivalent to the original \code{quantile()} function in
\proglang{R}. But in some sense, it should work well at a large scale.
The demo generates random data on 4 processors, then
utilizes the \code{mpi.quantile()}:
\begin{lstlisting}[language=rr,title=R Code]
mpi.quantile <- function(x.spmd, prob = 0.5){
  if(sum(prob < 0 | prob > 1) > 0){
    stop("prob should be in (0, 1)")
  }

  N <- allreduce(length(x.spmd), op = "sum")
  x.max <- allreduce(max(x.spmd), op = "max")
  x.min <- allreduce(min(x.spmd), op = "min")

  f.quantile <- function(x, prob = 0.5){
    allreduce(sum(x.spmd <= x), op = "sum") / N - prob
  }

  uniroot(f.quantile, c(x.min, x.max), prob = prob[1])$root
} # End of mpi.quantile().
\end{lstlisting}
Here, a numerical function is solved by using \code{uniroot()} to find out
the appropriate value where the cumulative probability is less than
or equal to the specified quantile.  This simple example shows that with just a little effort, direct MPI methods are greatly applicable on large scale data analysis and likelihood computing.

Note that in the way that the \code{uniroot()} call is used above, we are legitimately operating in parallel and on distributed
data.  Other optimization functions such as \code{optim()} and \code{nlm()}
can be utilized in the same way.


\section[Ordinary Least Squares]{Ordinary Least Squares}
\label{sec:ols}

\emph{Example:  Compute ordinary least square solutions for SPMD distributed data.}

The demo command is
\begin{lstlisting}
### At the shell prompt, run the demo with 4 processors by
### (Use Rscript.exe for windows system)
mpiexec -np 4 Rscript -e "demo(ols,'pbdDEMO',ask=F,echo=F)"
\end{lstlisting}

Ordinary least squares (OLS)
is perhaps \emph{the} fundamental tool of the statistician.  The goal is to find a solution $\bbeta$ such that
\begin{align}
\ltwo{ \bX\bbeta - \by }^2 \label{math:lls}
\end{align}
which is minimized.  In statistics, we tend to prefer to think of the problem as being of the form
\begin{align}
\by = \bX\bbeta + \bepsilon \label{math:statslls}
\end{align}
where $\by$ is $N\times 1$ observed vector,
$\bX$ is $N\times p$ designed matrix which is full rank and $N >> p$,
$\bbeta$ is the insterested parameters and unknown to be estimated,
and $\bepsilon$ is errors and to be minimized in norm.

Note that above, we do indeed mean (in fact, stress) \emph{a} solution to the linear least squares problem.  The full story is somewhat complicated.  The short explanation is that for many applications a statistician will face, expression (\ref{math:lls}) will actually have a unique solution.  But this is not always the case.  Indeed, it may occur that there is an infinite family of solutions.  So typically we go further and demand that a solution $\bbeta$ be such that $\ltwo{\bbeta}$ is at least as small as the corresponding norm of any other solution (although even this does not guarantee uniqueness).

A properly thorough treatment of the problems involved here go beyond the scope of this document, and require the reader have in-depth familiarity with linear algebra.  For our purposes, the concise explanation above will suffice.

The classical Maximum Likelihood solution is given by:
\begin{align}
 \hat{\bbeta} = (\bX^T\bX)^{-1}\bX^T\by \label{math:ols}
\end{align}
 
This example can be also generalized to weighted least square (WLS),
and linear mixed effect models (LME).

The implementation is straight forward:
\begin{lstlisting}[language=rr,title=R Code]
mpi.ols <- function(y.spmd, X.spmd){
  if(length(y.spmd) != nrow(X.spmd)){
    stop("length(y.spmd) != nrow(X.spmd)")
  }

  t.X.spmd <- t(X.spmd)
  A <- allreduce(t.X.spmd %*% X.spmd, op = "sum")
  B <- allreduce(t.X.spmd %*% y.spmd, op = "sum")

  solve(matrix(A, ncol = ncol(X.spmd))) %*% B
} # End of mpi.ols().

\end{lstlisting}

While this is a fine demonstration of the power of ``getting your hands dirty'', this approach is only efficient for small $N$ and small $p$.  Worse, directly computing the product
\begin{align*}
\bX^T\bX
\end{align*}
is often numerically non-stable.  Instead, it is generally better (although much slower) to take an orthogonal factorization of the data matrix.  Typically, the QR-decomposition is used to this end.  Here $\bX=QR$, where $Q$ is orthogonal and $R$ is upper trapezoidal.  This is beneficial, because orthogonal matrices are norm-preserving, and whence
\begin{align*}
\ltwo{\bX\bbeta - \by} &= \ltwo{QR\bbeta - \by}\\
  &= \ltwo{Q^TQR\bbeta - Q^T\by}\\
  &= \ltwo{R\bbeta - Q^T\by}
\end{align*}
The (arguably) much more well-known Singular Value Decomposition can also be used to develop yet another algebraically identical solution which is quite elegant.  Here, if we take $\bX = U\Sigma V^T$, then it can be shown that ``the'' desired solution is given by
\begin{align*}
\bbeta = V\Sigma^+ U^T\by
\end{align*}
where $\Sigma^+$ is the Moore-Penrose pseudoinverse of $\Sigma$.  However, this approach is handily the most computationally intensive.

The method utilizing QR to find a minimum norm solution has been implemented in \pkg{pbdDMAT} for objects of class \code{ddmatrix}.  For larger problems, and especially those where numerical accuracy is important, it may be more convenient to simply convert \code{y.spmd} and \code{X.spmd} into block-cyclic format as in the Part~\ref{part:dmat} and
to utilize \pkg{pbdBASE} and \pkg{pbdDMAT} for all matrix computation.

