
\chapter{Likelihood Function}
\label{chp:mle}


{\it
``It is a capital mistake to theorize before one has data.
Insensibly one begins to twist facts to suit theories,
instead of theories to suit facts.'' \\
\--- Sherlock Holmes
}


\section{Introduction}

We introduce general notations for likelihood function
which is a standard method for Parametric Statistics in
Statistical Inference~\citep{Casella2001}.
Two useful distributions are introduced.
The normal distribution additional to linear model is applied to
the example in Section~\ref{sec:ols}.
The multivariate normal
distribution\index{Distribution!multivariate normal distribution}
is also popular to model high dimensional data such as
model-based clustering in Chapter~\ref{chp:pmclust}.

Suppose $\bX = \{\bX_1, \bX_2, \ldots, \bX_N\}$ is a random sample,
which means independent identically distributed
(i.i.d.),\index{i.i.d.}
from a population which has a distribution $\mF(\btheta)$ with unknown
parameter $\btheta \in \bTheta$ where $\bTheta$ is the parameter space.
Suppose further $\mF$ has a probability density function
(pdf)\index{pdf}
$f(\bX_n; \btheta)$ provided an appropriate support.
The goal is to estimate $\btheta$ based on the observed data
$\bx = \{\bx_1, \bx_2, \ldots, \bx_N\}$.
Ideally, we want to infer what is the best candidate of $\btheta$
where $\bx$ is observed from.
Unlike in Mathematics, $\bx$ is known, but $\btheta$ is unknown
to be determined in Statistics.

Typically, a fancy way to estimate $\btheta$ is based on the
likelihood function of the data $\bx$
\begin{equation}
L(\btheta|\bx) = \prod_{n = 1}^N f(\bx_n; \btheta)
\label{eqn:likelihood}
\end{equation}
or the log likelihood function
\begin{equation}
\log L(\btheta|\bx) = \sum_{n = 1}^N \log f(\bx_n; \btheta).
\label{eqn:log_likelihood}
\end{equation}
Note that Equation~(\ref{eqn:log_likelihood}) has some
better properties for some distribution families and
is more numerically stable than Equation~(\ref{eqn:likelihood}).
We then maximize Equation~(\ref{eqn:log_likelihood})
over $\bTheta$ to obtain maximum likelihood estimation
(MLE)\index{MLE} $\hat{\btheta}_{ML}$ in either analytically or numerically.
See \citet{Casella2001} for details.




\section{Normal Distribution}

Section~\ref{sec:ols} is one way to find
$\btheta = \{\bbeta,\sigma^2\}$ for a linear model
without parametric assumption via ordinary least square estimator
$\hat{\btheta}_{ols} = \{\hat{\bbeta}_{ols},\hat{\sigma}_{ols}^2\}$.
An alternative way is based on likelihood approach by
assuming an identical normal distribution with mean zero and
variance $\sigma^2$ to the independent error terms of
Equation~(\ref{math:statslls}).
This implies a normal distribution\index{Distribution!normal distribution}
to the response $y_n$ for $n=1,2,\ldots, N$ that
\begin{equation}
y_n \stackrel{i.i.d}{\sim} N(\bx_n^\top\bbeta, \sigma^2)
\label{eqn:normal}
\end{equation}
where $\btheta = \{\bbeta, \sigma^2\}$, and
$\bbeta$ and $\bx_n$ has dimension $p\times 1$.

One may construct a log likelihood based on the normal density function as
\begin{equation}
\log L(\bbeta, \sigma^2|\by) = \sum_{n = 1}^N
\left[
-\frac{1}{2} \log (2\pi \sigma^2) -
\frac{(y_n - \bx_n^\top\bbeta)^2}{2\sigma^2}
\right].
\label{eqn:log_likelihood_normal}
\end{equation}
The MLEs
$\hat{\btheta}_{ML} = \{\hat{\beta}_{ML}, \hat{\sigma}^2_{ML}\}$
can be obtained analytically for this case by
taking the first derivatives of Equation~(\ref{eqn:log_likelihood_normal}),
setting them to zero, and
solving the equations.
The implementation for numerical solution for
Equation~(\ref{eqn:log_likelihood_normal}) is leaved in
Exercise~\ref{ex:likelihood1}.

The assumption of Equation~(\ref{eqn:normal}) limit the modeling capability.
We introduce a more general approach next for better
modeling.
Since the independent assumption and Multivariate Statistics, the
Equation~(\ref{eqn:normal})
implies a multivariate normal distribution
(MVN)\index{Distribution!multivariate normal distribution}\index{Distribution!MVN}
(introduced in Section~\ref{sec:mvn})
to response variables $\by$ with dimension $N\times 1$ that
\begin{equation}
\by \sim MVN_N(\bmu, \bSigma).
\label{eqn:mvn_n}
\end{equation}
where $\bmu = \bX\bbeta$ with length $N$,
$\bSigma = \sigma^2 \bI$ and $\bI$ is an $N\times N$ identity matrix.
In this case, the $\by$ has a density function as
\begin{equation*}
\displaystyle
\phi_N(\by; \bmu, \bSigma) =
(2\pi)^{-\frac{N}{2}} |\bSigma|^{-\frac{1}{2}}
e^{-\frac{1}{2} (\by - \bmu)^\top \bSigma^{-1} (\by - \bmu)}
\end{equation*}
and the log likelihood can reduce to
Equation~(\ref{eqn:log_likelihood_normal}).
The MLEs are
$\hat{\bbeta}_{ML} = (\bX^\top \bX)^{-1} \bX^\top \by$ and
$\sigma^2_{ML} = \frac{1}{N} (\by - \bar{y}\bone)^\top(\by - \bar{y}\bone)$
where $\bar{y}$ is the average of $\by$,
and $\bone$ is an one vector with length $N$.




\section{Multivariate Normal Distribution}
\label{sec:mvn}


Suppose $\{\bX_1,\bX_2,\ldots,\bX_N\}$ is a random sample from
multivariate normal distribution
(MVN)\index{Distribution!multivariate normal distribution}\index{Distribution!MVN}
\begin{equation}
\bX_n \stackrel{i.i.d}{\sim} MVN_p(\bmu, \bSigma)
\label{eqn:mvn_p}
\end{equation}
where $\btheta = \{\bmu, \bSigma\}$, $\bmu$ is a center with
dimension $p\times 1$, and $\bSigma$ is an $p\times p$ dispersion
matrix.
The $\bX_n$ has a density function as
\begin{equation*}
\displaystyle
\phi_p(\bx_n; \bmu, \bSigma) =
(2\pi)^{-\frac{p}{2}} |\bSigma|^{-\frac{1}{2}}
e^{-\frac{1}{2} (\bx_n - \bmu)^\top \bSigma^{-1} (\bx_n - \bmu)}
\end{equation*}
In general, $\bSigma$ could be an unstructured dispersion and positive
definite. Excepting over fitting problem,
an unstructured dispersion $\bSigma$ is desirable to
characterize correlation of dimensions since the estimation of
$\bSigma$ is completely supported by observed data.

Let $\bx = (\bx_1^\top, \bx_2^\top, \ldots, \bx_N^\top)^\top$
be an observed data matrix with dimension $N\times p$.
The log likelihood function for $N$ observations is
\begin{equation*}
\displaystyle
\log L(\bmu, \bSigma|\bx) = \sum_{n = 1}^N
-\frac{1}{2}
\left[
p\log(2\pi) + \log|\bSigma| + (\bx_n - \bmu)^\top \bSigma^{-1} (\bx_n - \bmu)
\right].
\label{eqn:log_likelihood_mvn_p}
\end{equation*}
The problem is the computing time grows as $N$ and $p$ increased.
In some numerically cases, such as model-based clustering
in Chapter~\ref{chp:pmclust}, the total log likelihood is
repeated computed in each iteration for all samples and all components.

Suppose $\bmu$ and $\bSigma$ are known, and no over- and under-flow,
an efficient way is given in next
\begin{lstlisting}[language=rr,title=R Code]
U <- chol(SIGMA)
logdet <- sum(log(abs(diag(U)))) * 2
B <- sweep(X.spmd, 2, MU) %*% backsolve(U, diag(1, p))

# The over- and under-flow need extral care after this step.
distval.spmd <- rowSums(B * B)

distval <- allreduce(sum(distval.spmd))
total.logL <- -(p * log(2 *pi) + logdet + distval) * 0.5
\end{lstlisting}
where \code{X.spmd} is a SPMD row-major matrix with dimension
\code{N.spmd} by \code{p}, \code{MU} is a vector with length \code{p}, and
\code{SIGMA} is a \code{p} by \code{p} positive definite matrix.
The sample size $N$ will be the sum of \code{N.spmd} across all processors.
Note that this trick of computing log likelihood is an one-pass implementation
of \code{X.spmd}, \code{MU}, and \code{SIGMA}.
See HPSC~\citep{hpsc2011} or \citet{gvl} for more details.




\section{Exercises}
\label{sec:mle_exercise}

\begin{enumerate}[label=\thechapter-\arabic*]

\item
What is the definition of ``independent identical distributed''?

\item
What is the definition of ``probability density function''?

\item
Suppose $g(\cdot)$ is a continuous function provided appropriate support,
argue that $g\left(\hat{\theta}_{ML}\right)$ is still a maximum likelihood
estimator of $g(\theta)$.

\item
Derive MLEs from Equation~(\ref{eqn:log_likelihood_normal}).

\item
As Exercise~\ref{ex:stat1}, argue that $\hat{\bbeta}_{ML}$ of
Equation~(\ref{eqn:log_likelihood_normal}) is also
an unbiased estimator of $\bbeta$.

\item
Argue that $\hat{\sigma}^2_{ML}$ of
Equation~(\ref{eqn:log_likelihood_normal})
is a biased estimator, but it is an asymptotic
unbiased estimator of $\sigma^2$.

\item
Assume data are stored in SPMD row-major matrix format,
implement an optimization function for
Equation~(\ref{eqn:log_likelihood_normal}), numerically optimized via
\code{optim()}\index{Code!\code{optim()}} in \proglang{R}.
Verify the results with the analytical solution.
\label{ex:likelihood1}

\item
Argue that Equation~(\ref{eqn:normal}) implies Equation~(\ref{eqn:mvn_n})
provided appropriated assumption hold.

\item
Give an example that $X$ and $Y$ are both have a normal distribution
but $(X, Y)$ is not a multivariate normal distribution.

\item
Give an example that $(X, Y)$ has a multivariate normal distribution,
but $X$ and $Y$ do not have an independent normal distribution.

\end{enumerate}

