\part{Reading and Managing Data}
\label{part:dmat}

%%% ----------------------------------------------------------------------
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Random Distributed Matrices}
\label{sec:reader}

The \pkg{pbdBASE} and \pkg{pbdDMAT} packages offer a distributed matrix class, \code{ddmatrix}, as well as a collection of high-level methods for performing common matrix operations.  For example, if you want to compute the mean of an \proglang{R} matrix \code{x}, you would call 
\begin{lstlisting}[language=rr]
mean(x)
\end{lstlisting}
That's exactly the same command you would issue if \code{x} is no longer an ordinary \proglang{R} matrix, but a distributed matrix.  These methods range from simple, embarrassingly parallel operations like sums and means, to tightly coupled linear algebra operations like matrix-matrix multiply and singular value decomposition.

Unfortunately, these higher methods come with a different cost:  getting the data into the right format, namely the distributed matrix class.  This can be especially frustrating because we assume that the any object of class \code{ddmatrix} is \emph{block cyclically distributed}.  This concept is discussed at length in the \pkg{pbdBASE} vignette \citep{Schmidt2012pbdBASEvignette}, and we do not intend to discuss the concept of a block cyclic data distribution at length herein.  However, we will demonstrate several examples of getting data into and out of the distributed block cyclic matrix format.

Once the hurdle of getting the data into the ``right format'' is out of the way, these methods offer very simple syntax (designed to mimic \proglang{R} as closely as possible) with the ability to scale computations on very large distributed machines.  So the process of getting the data into the correct format must be addressed.  We begin dealing with this issue in the simplest way possible, namely by using randomly generated data.  

\section{Fixed Global Dimension}\label{subsec:rng.gl}

\emph{Example:  randomly generate distributed matrices with random normal data of specificed global dimension.}

The demo command is
\begin{Command}
### At the shell prompt, run the demo with 4 processors by
### (Use Rscript.exe for windows system)
mpiexec -np 4 Rscript -e "demo(randmat_global,'pbdDEMO',ask=F,echo=F)"
\end{Command}

This demo shows 3 separate ways that one can generate a random normal matrix with specified global dimension.  The first two generate the matrix in full on at least one processor and distribute(s) the data, while the last method generates locally only what is needed.  As such, the first two can be considered demonstrations with what to do when you have data read in on one processor and need to distribute it out to the remaining processors, but for the purposes of building a randomly generated distributed matrix, they are not particularly efficient strategies.

The basic idea is as follows.  If we have a matrix \code{x} stored on processor 0 and \code{NULL} on the others, then we can distribute it out as an object of class \code{ddmatrix} via the command \code{as.ddmatrix()}.  For example
\begin{lstlisting}[language=rr]
if (comm.rank()==0){
  x <- matrix(rnorm(100), nrow=10, ncol=10)
} else {
  x <- NULL
}

dx <- as.ddmatrix(x)
\end{lstlisting}

will distribute the required data to the remaining processors.  We note for clarity that this is not equivalent to sending the full matrix to all processors and then throwing away all but what is needed.  Only the required data is communicated to the processors.

That said, having all of the data on all processors can be convenient while testing, if only for being more minimalistic in the amount of code/thinking required.  To do this, one need only do the following:

\begin{lstlisting}[language=rr]
x <- matrix(rnorm(100), nrow=10, ncol=10)

dx <- as.ddmatrix(x)
\end{lstlisting}

Here, each processor generates the full, global matrix, then throws away what is not needed.  Again, this is not efficient, but the code is concise, so it is extremely useful in testing.  Now, this assumes you are using the same seed on each processor.  This can be managed using the \pkg{pbdMPI} function \code{comm.set.seed()}, as in the demo script.  For more information, see that package's documentation.

Finally, you can generate locally only what you need.  The demo script does this via the \pkg{pbdDEMO} package's \code{Hnorm()} or ``huge normal'' function.  There are two others provided, namely \code{Hconst()} and \code{Hunif()}.  The naming convention was chosen because the latter most function name makes the author laugh.  Internally, these ``huge'' functions rely on a much stronger working knowledge of the underlying data structure than most will be comfortable with.  However, for the sake of completeness, we will briefly examine \code{Hnorm()}.

\begin{lstlisting}[language=rr,title=Hnorm()]
Hnorm <- function(dim, bldim, mean=0, sd=1, ICTXT=0)
{
  if (length(bldim)==1L)
    bldim <- rep(bldim, 2L)
  
  ldim <- base.numroc(dim=dim, bldim=bldim, ICTXT=ICTXT, fixme=FALSE)
    
  if (any(ldim < 1L)){
    xmat <- matrix(0)
    ldim <- c(1, 1)
  }
  else
    xmat <- matrix(rnorm(prod(ldim), mean=mean, sd=sd), nrow=ldim[1L], ncol=ldim[2L])
              
  dx <- new("ddmatrix", Data=xmat,
            dim=dim, ldim=ldim, bldim=bldim, CTXT=ICTXT)
            
  return(dx)
}
\end{lstlisting}

The concise explanation is that the \code{base.numroc()} utility determines the size of the local storage.  This is all very well documented in the \pkg{pbdBASE} documentation, but since no one even pretends to read that stuff, \texttt{NUMROC} is a ScaLAPACK tool, which means ``\texttt{NUM}ber of \texttt{R}ows \texttt{O}r \texttt{C}olumns.''  The function \code{base.numroc()} is an implementation in \proglang{R} which calculates the number of rows \emph{and} columns at the same time (so it is a bit of a misnomer, but preserved for historical reasons).  

More precisely, it calculates the local storage requirements given a global dimension \code{dim}, a blocking factor \code{bldim}, and a BLACS context number \code{ICTXT}.  The extra argument \code{fixme} determines whether or not the lowest value returned should be 1.  If \code{fixme==FALSE} and any of the returned local dimensions are less than 1, then that processor does not actually own any of the global matrix --- it has no local storage.  But something must be stored, and so we default this to \code{matrix(0)}, the $1\times 1$ matrix with single entry 0.








\section{Diagonal, Fixed Global Dimension}

\emph{Example:  randomly generate \textbf{diagonal} distributed matrices with random normal data of specificed global dimension.}

The demo command is
\begin{Command}
### At the shell prompt, run the demo with 4 processors by
### (Use Rscript.exe for windows system)
mpiexec -np 4 Rscript -e "demo(randmat_diag_global,'pbdDEMO',ask=F,echo=F)"
\end{Command}

In \proglang{R}, the \code{diag()} function serves two purposes.  If given a matrix, it produces a vector containing the diagonal entries of that matrix; but if given a vector, it constructs a diagonal matrix whose diagonal is that vector.  And so for example, the zero and identity matrices of any dimension can quickly be constructed via:
\begin{lstlisting}[language=rr,title=Diagonal Matrices in R]
diag(x=0, nrow=10, ncol=10) # zero matrix
diag(x=1, nrow=10, ncol=10) # identity matrix
\end{lstlisting}

Both of the above functionalities of \code{diag()} are available for distributed matrices; however we will only focus on the latter. 

When you wish to construct a diagonal distributed matrix, you can easily do so by using the additional \code{type=} argument to our \code{diag()} method.  By default, \code{type="matrix"}, though the user may specify \code{type="ddmatrix"}.  If so, then as one might expect, the optional \code{bldim=} and \code{ICTXT=} arguments are available.  So with just a little bit of tweaking, the above example becomes:
\begin{lstlisting}[language=rr,title=Diagonal Matrices in pbdR]
diag(x=0, nrow=10, ncol=10, type="ddmatrix") # zero (distributed) matrix
diag(x=1, nrow=10, ncol=10, type="ddmatrix") # identity (distributed) matrix
\end{lstlisting}
In fact, the \code{type=} argument employs partial matching, so if we really want to be lazy, then we could simply do the following:
\begin{lstlisting}[language=rr,title=Diagonal Matrices in pbdR]
diag(x=0, nrow=10, ncol=10, type="d") # zero (distributed) matrix
diag(x=1, nrow=10, ncol=10, type="d") # identity (distributed) matrix
\end{lstlisting}

Beyond the above brief explanation, the demo for this functionality is mostly self-contained, although we do employ the \code{redistribute()} function to fully show off local data storage.  This function is explained in detail in Chapter~\ref{sec:redist}.



\section{Fixed Local Dimension}

\emph{Example:  randomly generate distributed matrices with random normal data of specificed local dimension.}

The demo command is
\begin{Command}
### At the shell prompt, run the demo with 4 processors by
### (Use Rscript.exe for windows system)
mpiexec -np 4 Rscript -e "demo(randmat_local,'pbdDEMO',ask=F,echo=F)"
\end{Command}

This is similar to the above, but with a critical difference.  Instead of specifying a fixed \emph{global} dimension and then go determine what the local storage space is, instead we specify a fixed \emph{local} dimension and then go figure out what the global dimension should be.  This can be useful for testing weak scaling of an algorithm, where different numbers of cores are used with the same local problem size.

To this end, the demo script utilizes the \code{Hnorm.local()} function, which has the user specify a local dimension size that all the processors should use, as well as a blocking factor and BLACS context value.  

\begin{lstlisting}[language=rr,title=Hnorm.local()]
Hnorm.local <- function(ldim, bldim, mean=0, sd=1, ICTXT=0)
{
  if (length(bldim)==1L)
    bldim <- rep(bldim, 2L)
  
  blacs_ <- base.blacs(ICTXT=ICTXT)
  nprows <- blacs_$NPROW
  npcols <- blacs_$NPCOL
  
  dim <- c(nprows*ldim[1L], npcols*ldim[2L])
  
  if (any( (dim %% bldim) != 0 )){
    comm.cat("WARNING : at least one margin of 'bldim' does not divide the global dimension.\n", quiet=T)
    
    bldim[1L] <- nbd(dim[1L], bldim[1L])
    bldim[2L] <- nbd(dim[2L], bldim[2L])
    comm.cat(paste("Using bldim of ", bldim[1L], "x", bldim[2L], "\n\n", sep=""), quiet=T)
  }
  
  Data <- matrix(rnorm(prod(ldim), mean=mean, sd=sd), nrow=ldim[1L], ncol=ldim[2L])
  
  dx <- new("ddmatrix", Data=Data,
            dim=dim, ldim=ldim, bldim=bldim, CTXT=ICTXT)
  
  return(dx)
}
\end{lstlisting}

Now here things get somewhat tricky, because in order for this matrix to exist at all, each margin of the blocking factor must divide (as an integer) the corresponding margin of the global dimension.  To better understand why this is so, the reader is suggested to read the \pkg{pbdBASE} vignette.  But if you already understand or are merely willing to take it on faith, then you surely grant that this is a problem.

So here, we assume that the local dimension is chosen appropriately, but it is possible that a bad blocking factor is supplied by the user.  Rather than halt with a stop error, we attempt to find the next best blocking factor possible.  We do this with a simple ``next best divisor'' function:

\begin{lstlisting}[language=rr,title=nbd()]
nbd <- function(n, d)
{
  if (n < d)
    stop("'n' may not be smaller than 'd'")
  
  ret <- .Fortran("NBD", 
                  as.integer(n), as.integer(d),
                  PACKAGE="pbdDEMO")$D
  
  return( ret )
}
\end{lstlisting}

which is just a shallow wrapper on the \proglang{Fortran} code:

\begin{lstlisting}[language=ft,title=NBD]
      SUBROUTINE NBD(N, D)
      INTEGER N, D, I, TEST
      
      DO 10 I = D+1, N-1, 1
        TEST = MOD(N, I)
        IF (TEST.EQ.0) THEN
          D = I
          RETURN
        END IF
   10 CONTINUE
      
      D = N
      RETURN
      END
\end{lstlisting}

Even those who don't know \proglang{Fortran} should easily be able to see what is going on here.  We are given integers \code{N} and \code{D}, and we loop over the integers inbetween these two until we find one which divides \code{N}.

So going back to the \code{Hnorm.local()} function, the second \code{if} block contains the readjusting (as necessary) of the blocking factors.  Then the local data matrix is generated and wrapped up in its class before being returned --- everything else is just sugar.