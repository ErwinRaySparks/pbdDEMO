\part{Ad Hoc Methods}

\section[Statistics Examples]{Statistics Examples}
\label{sec:statistics_examples}
\addcontentsline{toc}{section}{\thesection. Statistics Examples}

This section introduces four simple examples and explains a little about
distributed data computing. These implemented functions are
selected from the Cookbook of HPSC website~\ref{hpsc2012} at
\url{http://thirteen-01.stat.iastate.edu/snoweye/hpsc/?item=cookbook}.
Please see more details there.

{\color{red} \bf Warning:}
We presume that readers have idea about SPMD programming. If not,
please read pbdMPI's vignette~\cite{Chen2012pbdMPIvignette} first.
If possible, readers are encouraged to run the demo of pbdMPI package
and go through the code step by step.




\subsection[Sample Mean and Sample Variance]{Sample Mean and Sample Variance}
\label{sec:sample_stat}
\addcontentsline{toc}{subsection}{\thesubsection. Sample Mean and Sample Variance}

Suppose $\bx = \{x_1, x_2, \ldots, x_N\}$
are observed samples, and $N$ is very large.
We can distribute $\bx$ in 2 processors, and each processor roughly
takes half of data. One simple way to compute sample mean $\bar{x}$ and
sample variance $s_x$ is based on the formulas:
% $$
% \begin{array}{rcl}
% \bar{x} & = & \displaystyle\frac{1}{N} \sum_{n = 1}^N x_n \\[.3cm]
%         & = & \displaystyle\sum_{n = 1}^N \frac{x_n}{N} \\[.7cm]
%        \text{and}&&\\[.4cm]
% s_x     & = & \displaystyle\frac{1}{N - 1} \sum_{n = 1}^N (x_n - \bar{x})^2 \\[.3cm]
%         & = & \displaystyle\sum_{n = 1}^N \frac{x^2_n}{N-1} - \frac{N \bar{x}^2}{N-1} \\
% \end{array}
% $$
\begin{align*}
\bar{x} &= \frac{1}{N} \sum_{n = 1}^N x_n \\[.3cm]
        &= \sum_{n = 1}^N \frac{x_n}{N}
\end{align*}
and
\begin{align*}
s_x     &= \frac{1}{N - 1} \sum_{n = 1}^N (x_n - \bar{x})^2 \\[.3cm]
        &= \sum_{n = 1}^N \frac{x^2_n}{N-1} - \frac{N \bar{x}^2}{N-1}
\end{align*}


The demo command is
\begin{Command}
### At the shell prompt, run the demo with 2 processors by
### (Use Rscript.exe for windows system)
mpiexec -np 2 Rscript -e "demo(sample_stat,'pbdDEMO',ask=F,echo=F)"
\end{Command}

The demo \code{sample_stat} generates fake data on 2 processors, then
utilize \code{mpi.stat} function as
\begin{Code}[title=R Code]
mpi.stat <- function(x.spmd){
  ### For mean(x).
  N <- allreduce(length(x.spmd), op = "sum")
  bar.x.spmd <- sum(x.spmd / N)
  bar.x <- allreduce(bar.x.spmd, op = "sum")

  ### For var(x).
  s.x.spmd <- sum(x.spmd^2 / (N - 1))
  s.x <- allreduce(s.x.spmd, op = "sum") - bar.x^2 * (N / (N - 1))

  list(mean = bar.x, s = s.x)
} # End of mpi.stat().
\end{Code}
where \code{allreduce} in \pkg{pbdMPI}~\citep{Chen2012pbdMPIpackage} can
be utilized in this examples to aggregate local information across
all processors.

Note that we tend to use suffix \code{.spmd} to indicate a distributed
local object which is a portion of big object.
We also tend to use common variables without subfix \code{.spmd}
since SPMD programming.
In SPMD case, it may be a redundant idea to invent a \code{spmd} S4 class
or methods for this purpose. The S3 class and methods are sufficient
and fast in most SPMD case.
In contrast, it is a better idea to invent a \code{ddmatrix} class as in
pbdBASE~\citep{Schmidt2012pbdBASEpackage} and
pbdDMAT~\citep{Schmidt2012pbdDMATpackage}, then let efficient libraries
handle computing and avoid tedious coding.
A silly example can be found in the Section~{sec:ols}.




\subsection[Binning]{Binning}
\label{sec:binning}
\addcontentsline{toc}{subsection}{\thesubsection. Binning}

Binning is a classical statistics and can quickly summarize
the data structure by setting some breaks between max and min of data.
The demo command is
\begin{Command}
### At the shell prompt, run the demo with 2 processors by
### (Use Rscript.exe for windows system)
mpiexec -np 2 Rscript -e "demo(binning,'pbdDEMO',ask=F,echo=F)"
\end{Command}

The demo \code{binning} generates fake data on 2 processors, then
utilize \code{mpi.bin} function as
\begin{Code}[title=R Code]
mpi.bin <- function(x.spmd, breaks = pi / 3 * (-3:3)){
  bin.spmd <- table(cut(x.spmd, breaks = breaks))
  bin <- as.array(allreduce(bin.spmd, op = "sum"))
  dimnames(bin) <- dimnames(bin.spmd)
  class(bin) <- class(bin.spmd)
  bin
} # End of mpi.bin().
\end{Code}
An easy implementation is to utilize \code{table} function to obtain
local counts, then call \code{allreduce} to obtain global counts.



\subsection[Quantile]{Quantile}
\label{sec:quantile}
\addcontentsline{toc}{subsection}{\thesubsection. Quantile}

Quantile is the other useful tool from fundamental statistics
which provides data distribution for given quantile.
The demo code is
\begin{Command}
### At the shell prompt, run the demo with 2 processors by
### (Use Rscript.exe for windows system)
mpiexec -np 2 Rscript -e "demo(quantile,'pbdDEMO',ask=F,echo=F)"
\end{Command}

This is only an implicit implementation to approximate a quantile
and is not equivalent to the original \code{quantile} function in
\proglang{R}. But in some sense, it should work well in large scale.
The demo \code{quantile} generates fake data on 2 processors, then
utilizes \code{mpi.quantile} function as
\begin{Code}[title=R Code]
mpi.quantile <- function(x.spmd, prob = 0.5){
  if(sum(prob < 0 | prob > 1) > 0){
    stop("prob should be in (0, 1)")
  }

  N <- allreduce(length(x.spmd), op = "sum")
  x.max <- allreduce(max(x.spmd), op = "max")
  x.min <- allreduce(min(x.spmd), op = "min")

  f.quantile <- function(x, prob = 0.5){
    allreduce(sum(x.spmd <= x), op = "sum") / N - prob
  }

  uniroot(f.quantile, c(x.min, x.max), prob = prob[1])\$root
} # End of mpi.quantile().
\end{Code}
where a numerical function is solved by \code{uniroot} to find out
the appropriate value such that cumulated probability is less than
or equal to the specified quantile.

This simple example shows that the SPMD is greatly applicable on large
scale data analysis and likelihood computing.
Note that the \code{uniroot} call is working in parallel and on distributed
data, i.e. other optimization functions can work the same way,
since SPMD simly assumes every processors do the same work
simulatinuously.




\subsection[Ordinary Least Square]{Ordinary Least Square}
\label{sec:ols}
\addcontentsline{toc}{subsection}{\thesubsection. Ordinary Least Square}

This is a fundament tool to find a solution for
$$\by = \bX\bbeta + \bepsilon$$
where $\by$ is $N\times 1$ observed vector,
$\bX$ is $N\times p$ designed matrix which is full rank and $N >> p$,
$\bbeta$ is the insterested parameters and unknown to be estimated,
and $\bepsilon$ is errors and to be minimized.
A classical solution is to
$$
\hat{\bbeta} = (\bX^t\bX)^{-1}\bX^t\by
$$

The demo code is
\begin{Command}
### At the shell prompt, run the demo with 2 processors by
### (Use Rscript.exe for windows system)
mpiexec -np 2 Rscript -e "demo(ols,'pbdDEMO',ask=F,echo=F)"
\end{Command}

The implementation is straight forward as
\begin{Code}[title=R Code]
mpi.ols <- function(y.spmd, X.spmd){
  if(length(y.spmd) != nrow(X.spmd)){
    stop("length(y.spmd) != nrow(X.spmd)")
  }

  t.X.spmd <- t(X.spmd)
  A <- allreduce(t.X.spmd %*% X.spmd, op = "sum")
  B <- allreduce(t.X.spmd %*% y.spmd, op = "sum")

  solve(matrix(A, ncol = ncol(X.spmd))) %*% B
} # End of mpi.ols().

\end{Code}
Note that this is a silly implementation for demonstrations and
explain fundamental idea of OLS.
This is only efficient for small $N$ and small $p$. For larger scale,
we suggest to simply convert \code{y.spmd} and \code{X.spmd} into
block-cyclic format as in the Section~\ref{sec:spmd2dmat} and
to utilize pbdBASE and pbdDMAT for all matrix computation.

